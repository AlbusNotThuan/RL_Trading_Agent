{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InWZ9eV3ONXS",
        "outputId": "ef8f4fc9-afef-40f3-804b-7daa00d0646b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (4.25.6)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX #Install tensorboardX module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSexRhGTOT_y",
        "outputId": "bf3aab68-9077-4bfe-cc09-9e5633b0e01b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/nandodmelo/cripto-hour?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 25.5M/25.5M [00:01<00:00, 20.8MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "nandodmelo_cripto_hour_path = kagglehub.dataset_download('nandodmelo/cripto-hour')\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DwHgVytQOV-k"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Softmax, Concatenate, Reshape\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "from datetime import datetime\n",
        "from collections import deque\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorboardX import SummaryWriter\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Conv1D,Conv2D, MaxPooling1D, Activation, Concatenate, LSTM, ZeroPadding2D #, LSTM\n",
        "#from tensorflow.compat.v1.keras.layers import CuDNNLSTM as LSTM #This line was removed, because CuDNNLSTM is not supported in TensorFlow 2.x\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1r5hkbCOZ1x",
        "outputId": "e3fee859-858f-4f9c-a756-2145f0d542ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/XRPUSDT_norm.csv\n",
            "/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/BTCUSDT_norm.csv\n",
            "/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/ETHUSDT_norm.csv\n",
            "/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/BNBUSDT.csv\n",
            "/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/XRPUSDT.csv\n",
            "/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/BTCUSDT.csv\n",
            "/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/BNBUSDT_norm.csv\n",
            "/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/ETHUSDT.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk(nandodmelo_cripto_hour_path):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ytKfl2DSOyOC"
      },
      "outputs": [],
      "source": [
        "K.set_image_data_format(\"channels_first\")\n",
        "\n",
        "random.seed(2002)\n",
        "np.random.seed(32)\n",
        "tf.random.set_seed(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OakDQSsdOafl"
      },
      "outputs": [],
      "source": [
        "class CustomEnv:\n",
        "    def __init__(self, df, df_normalized, initial_balance=1000, stocks=['USDCUSDT','BTCUSDT','BNBBTC','BNBBTC'],lookback_window_size=50, model=''):\n",
        "        # Define action space and state size and other custom parameters\n",
        "        self.xarray = df_normalized\n",
        "        self.df = df\n",
        "        self.df_total_steps = self.xarray.shape[0]\n",
        "        self.initial_balance = initial_balance\n",
        "        self.lookback_window_size = lookback_window_size\t\t# Historical data window\n",
        "        self.normalize_value = 40000\t\t#Value to normalize transaction data\n",
        "        self.model = model\n",
        "\n",
        "        self.weights = [1]+[0]*(self.xarray.shape[2]-1)\t\t# Initial Weights\n",
        "        self.quants = [0]*self.xarray.shape[2] \t\t#Initial quantities\n",
        "        self.quants_ubah = [0]*self.xarray.shape[2]\t\t# Initial quantities for buy and hold\n",
        "\n",
        "\n",
        "        self.cash = 0 # Amout of cash\n",
        "\n",
        "        self.stocks =  stocks \t# list of assets\n",
        "        self.market_state = dict.fromkeys(self.stocks)\t\t# Dict for each asset\n",
        "\n",
        "\n",
        "        #Initial amount of money to Buy n hold\n",
        "        self.ubah = initial_balance\n",
        "\n",
        "\n",
        "        #Deque for Order History\n",
        "        self.orders_history = deque(maxlen=self.lookback_window_size)\n",
        "        self.market_history = deque(maxlen=self.lookback_window_size)  # Market history contains the OHCL/Technical Features values for the last lookback_window_size prices (open, high, close, low)\n",
        "\n",
        "\n",
        "    # Reset the state of the environment to an initial state\n",
        "    def reset(self, env_steps_size = 0):\n",
        "        self.balance = self.initial_balance\n",
        "        self.net_worth = self.initial_balance\n",
        "        self.prev_net_worth = self.initial_balance\n",
        "        self.weights = [1]+[0]*(self.xarray.shape[2]-1)\n",
        "        self.quants = [0]*self.xarray.shape[2]\n",
        "        self.quants_ubah = [0]*self.xarray.shape[2]\n",
        "        self.short_sell = [1,1,1]\n",
        "        self.cash = self.initial_balance\n",
        "        self.ubah = self.initial_balance\n",
        "\n",
        "\n",
        "\n",
        "        if env_steps_size > 0: # used for training dataset\n",
        "            # Randomly selects a value contained between the initial size of the dataset and the final size minus the number of steps.\n",
        "            self.start_step = random.randint(self.lookback_window_size, self.df_total_steps -env_steps_size)\n",
        "            self.end_step = self.start_step + env_steps_size\n",
        "        else: # used for testing dataset\n",
        "            # Randomly selects a value contained between the initial size of the dataset and the final size minus the number of steps.\n",
        "            self.start_step = random.randint(self.lookback_window_size, self.df_total_steps -env_steps_size)\n",
        "            self.end_step = self.start_step + env_steps_size\n",
        "\n",
        "\n",
        "        self.current_step = self.start_step #Define initial Step\n",
        "        self.quants_ubah = [(self.initial_balance/len(self.weights))/ np.array([self.df[self.current_step,2,x] for x in range(0,len(self.stocks))])]    # Defines the quantities for buy n hold\n",
        "\n",
        "        #Append the data from end t beginning\n",
        "        for i in reversed(range(self.lookback_window_size)):\n",
        "            current_step = self.current_step - i\n",
        "            self.orders_history.append([self.net_worth/self.normalize_value,\n",
        "                          self.cash/self.normalize_value] +\n",
        "                          [number for number in self.quants] +\n",
        "                          [number for number in self.weights])\n",
        "\n",
        "        #Append the data from end t beginning for each asset\n",
        "        for j in range(0,len(self.stocks)):\n",
        "            self.market_state[str(j)] = deque(maxlen=self.lookback_window_size)\n",
        "\n",
        "            for i in reversed(range(self.lookback_window_size)):\n",
        "              current_step = self.current_step - i\n",
        "              self.market_state[str(j)].append(self.xarray[current_step, :,j])\n",
        "\n",
        "        #The model is EIIE type, the state will contain only asset-related data, transaction history data, is added separately in the network.\n",
        "        if self.model == \"EIIE\":\n",
        "            state = np.stack(([self.market_state[str(x)] for x in range(0,len(self.stocks))]))\n",
        "        else:\n",
        "            state = np.concatenate(([self.market_state[str(x)] for x in range(0,len(self.stocks))]), axis=1)\n",
        "            state = np.concatenate((state, self.orders_history) , axis=1)\n",
        "\n",
        "        return state, self.orders_history\n",
        "\n",
        "\n",
        "\n",
        "    # Get the data points for the given current_step\n",
        "    def _next_observation(self):\n",
        "        start = time.time()\n",
        "        # In this step, it updates the state with the most recent point that was used in 'step', for example, in Step it takes the next point after the market history, so if the market history goes to t, in the step it takes the point t+1, in the next observation it appends this point.\n",
        "        for j in range(0, len(self.stocks)):\n",
        "          self.market_state[str(j)].append(self.xarray[self.current_step, :, j])\n",
        "\n",
        "\n",
        "        if self.model == \"EIIE\":\n",
        "            obs = np.stack(([self.market_state[str(x)] for x in range(0,len(self.stocks))]))\n",
        "        else:\n",
        "            obs = np.concatenate(([self.market_state[str(x)] for x in range(0,len(self.stocks))]), axis=1)\n",
        "            obs = np.concatenate((obs, self.orders_history) , axis=1)\n",
        "\n",
        "        return obs\n",
        "\n",
        "\n",
        "\n",
        "    # Execute one time step within the environment\n",
        "    def step(self, prediction):\n",
        "        # Use to calculate the transactions fee\n",
        "        prices_ant =  np.array([self.df[self.current_step,2,x] for x in range(0,len(self.stocks))])\n",
        "        # One step on env\n",
        "        self.current_step += 1\n",
        "\n",
        "\n",
        "        # Get the prices in the current step\n",
        "        prices = np.array([self.df[self.current_step,2,x] for x in range(0,len(self.stocks))])\n",
        "\n",
        "        #Calculates the balance considering the quantities purchased in the previous step, and the prices at the current time\n",
        "        self.balance = self.cash + np.dot(prices[1:],self.quants[1:])\n",
        "\n",
        "\n",
        "        # Use to calculate the transactions fee\n",
        "        quants_ant = self.quants\n",
        "\n",
        "        #Get the quantities, considering the current values and the balance of the previous transaction\n",
        "        self.quants = [self.balance*prediction[x]/prices[x] for x in range(0,len(self.stocks))]\n",
        "\n",
        "        # Calculate the tax of buying and selling, 10% of the difference between quants of the periods\n",
        "        # 0,1% is the binance tax source\n",
        "        tax = np.sum(abs(np.dot(np.array(self.quants),prices) - np.dot(np.array(quants_ant),prices_ant)))*0.001\n",
        "\n",
        "\n",
        "        #See the value of the cash term(Stable currency, in the future consider whether this approach is valid)\n",
        "        self.cash = self.quants[0]*prices[0]\n",
        "\n",
        "        #Save the previous net worth\n",
        "        self.prev_net_worth = self.net_worth\n",
        "\n",
        "\n",
        "        #Calculate the new portfolio value\n",
        "        self.net_worth = np.dot(self.quants,prices) - tax\n",
        "\n",
        "\n",
        "        #Append the transactions values to deque\n",
        "        self.orders_history.append([self.net_worth/self.normalize_value,\n",
        "                      self.cash/self.normalize_value] +\n",
        "                      [number/self.normalize_value for number in self.quants] + prediction.tolist())\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = np.log(self.net_worth/self.prev_net_worth)\n",
        "        #reward = self.net_worth - self.prev_net_worthh\n",
        "\n",
        "        if self.net_worth <= self.initial_balance/2:\n",
        "          done = True\n",
        "        else:\n",
        "          done = False\n",
        "        obs = self._next_observation()\n",
        "\n",
        "\n",
        "        return obs, self.orders_history, reward, done, prices\n",
        "\n",
        "    # render environment\n",
        "    def render(self):\n",
        "        print(f'Step: {self.current_step}, Net Worth: {self.net_worth}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qKD3-FvyVLgX"
      },
      "outputs": [],
      "source": [
        "class CustomAgent:\n",
        "    # A custom Bitcoin trading agent\n",
        "    def __init__(self, lookback_window_size=50, lr=0.00005, epochs=1, stocks=[], optimizer=Adam, batch_size=32, model='', shape = [],depth=0, comment=\"\"):\n",
        "        self.lookback_window_size = lookback_window_size\n",
        "        self.comment = comment\n",
        "        self.depth = depth\n",
        "        self.stocks = stocks\n",
        "        self.shape = shape\n",
        "        self.model = model\n",
        "\n",
        "        # Action Space it goes from 0 to the number of assets in the portfolio\n",
        "        self.action_space = np.array(range(0,len(self.stocks)))\n",
        "\n",
        "        # Create a folder to save models\n",
        "        self.log_name = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")+\"_Crypto_trader\"\n",
        "\n",
        "        # State size contains Market+Orders+Indicators history for the last lookback_window_size steps\n",
        "        if self.model ==\"EIIE\":\n",
        "            self.state_size = (len(stocks), lookback_window_size, self.shape[1])\n",
        "        else:\n",
        "            self.state_size = (lookback_window_size, self.shape[1]*self.shape[2]+2+2*self.shape[2]) # 5 standard OHCL information + market and indicators\n",
        "\n",
        "        # Neural Networks part\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.optimizer = optimizer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Create shared Actor-Critic network model\n",
        "        self.Actor = self.Critic = Shared_Model(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer, model=self.model)\n",
        "\n",
        "\n",
        "\n",
        "    # create tensorboard writer\n",
        "    def create_writer(self, initial_balance, normalize_value, train_episodes):\n",
        "        self.replay_count = 0\n",
        "        self.writer = SummaryWriter('runs/'+self.log_name)\n",
        "\n",
        "        # Create folder to save models\n",
        "        if not os.path.exists(self.log_name):\n",
        "          os.makedirs(self.log_name)\n",
        "\n",
        "        self.start_training_log(initial_balance, normalize_value, train_episodes)\n",
        "\n",
        "    def start_training_log(self, initial_balance, normalize_value, train_episodes):\n",
        "        # save training parameters to Parameters.json file for future\n",
        "        current_date = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
        "        params = {\n",
        "          \"training start\": current_date,\n",
        "          \"initial balance\": initial_balance,\n",
        "          \"training episodes\": train_episodes,\n",
        "          \"lookback window size\": self.lookback_window_size,\n",
        "          \"depth\": self.depth,\n",
        "          \"lr\": self.lr,\n",
        "          \"epochs\": self.epochs,\n",
        "          \"batch size\": self.batch_size,\n",
        "          \"normalize value\": normalize_value,\n",
        "          \"model\": self.model,\n",
        "          \"comment\": self.comment,\n",
        "          \"saving time\": \"\",\n",
        "          \"Actor name\": \"\",\n",
        "          \"Critic name\": \"\",\n",
        "        }\n",
        "        with open(self.log_name+\"/Parameters.json\", \"w\") as write_file:\n",
        "          json.dump(params, write_file, indent=4)\n",
        "\n",
        "\n",
        "    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.95, normalize=True):\n",
        "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
        "        deltas = np.stack(deltas)\n",
        "        gaes = copy.deepcopy(deltas)\n",
        "        for t in reversed(range(len(deltas) - 1)):\n",
        "          gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
        "\n",
        "        target = gaes + values\n",
        "        if normalize:\n",
        "          gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
        "        return np.vstack(gaes), np.vstack(target)\n",
        "\n",
        "    def replay(self, states,orders,  rewards, predictions, dones, next_states, orders_history):\n",
        "        # reshape memory to appropriate shape for training\n",
        "        states = np.vstack(states)\n",
        "        order = np.vstack(orders)\n",
        "        next_states = np.vstack(next_states)\n",
        "        orders_history =  np.vstack(orders_history)\n",
        "\n",
        "        if self.model == \"EIIE\":\n",
        "            values = self.Critic.critic_predict(states, np.expand_dims(order, axis=1))\n",
        "        else:\n",
        "            values = self.Critic.critic_predict(states, np.expand_dims(np.expand_dims(order, axis=0), axis=0))\n",
        "\n",
        "        predictions = np.vstack(predictions)\n",
        "        next_values = self.Critic.critic_predict(next_states, np.expand_dims(orders_history, axis=1))\n",
        "\n",
        "        # Compute advantages\n",
        "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
        "        '''\n",
        "        plt.plot(target,'-')\n",
        "        plt.plot(advantages,'.')\n",
        "        ax=plt.gca()\n",
        "        ax.grid(True)\n",
        "        plt.show()\n",
        "        '''\n",
        "        # Stack everything to numpy array\n",
        "        y_true = np.hstack([advantages, predictions])\n",
        "\n",
        "\n",
        "        # training Actor and Critic networks\n",
        "        if self.model == \"EIIE\":\n",
        "            a_loss = self.Actor.Actor.fit([states,np.expand_dims(order, axis=1)], y_true, epochs=self.epochs, verbose=0, shuffle=True, batch_size=self.batch_size)\n",
        "            c_loss = self.Critic.Critic.fit([states,np.expand_dims(order, axis=1)], target, epochs=self.epochs, verbose=0, shuffle=True, batch_size=self.batch_size)\n",
        "        else:\n",
        "            a_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=True, batch_size=self.batch_size)\n",
        "            c_loss = self.Critic.Critic.fit(states, target, epochs=self.epochs, verbose=0, shuffle=True, batch_size=self.batch_size)\n",
        "\n",
        "        self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
        "        self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
        "        self.replay_count += 1\n",
        "\n",
        "        return np.sum(a_loss.history['loss']), np.sum(c_loss.history['loss'])\n",
        "\n",
        "    def act(self, state, order):\n",
        "        # Use the network to predict the next action to take, using the model\n",
        "        prediction = self.Actor.actor_predict(np.expand_dims(state, axis=0), np.expand_dims(np.expand_dims(order, axis=0), axis=0))[0]\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def save(self, name=\"Crypto_trader\", score=\"\", args=[]):\n",
        "        # save keras model weights\n",
        "        self.Actor.Actor.save_weights(f\"{self.log_name}/{score}_{name}_Actor.weights.h5\")\n",
        "        self.Critic.Critic.save_weights(f\"{self.log_name}/{score}_{name}_Critic.weights.h5\")\n",
        "\n",
        "        # update json file settings\n",
        "        if score != \"\":\n",
        "          with open(self.log_name+\"/Parameters.json\", \"r\") as json_file:\n",
        "            params = json.load(json_file)\n",
        "          params[\"saving time\"] = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
        "          params[\"Actor name\"] = f\"{score}_{name}_Actor.h5\"\n",
        "          params[\"Critic name\"] = f\"{score}_{name}_Critic.h5\"\n",
        "          with open(self.log_name+\"/Parameters.json\", \"w\") as write_file:\n",
        "            json.dump(params, write_file, indent=4)\n",
        "\n",
        "        # log saved model arguments to file\n",
        "        if len(args) > 0:\n",
        "          with open(f\"{self.log_name}/log.txt\", \"a+\") as log:\n",
        "            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            arguments = \"\"\n",
        "            for arg in args:\n",
        "              arguments += f\", {arg}\"\n",
        "            log.write(f\"{current_time}{arguments}\\n\")\n",
        "\n",
        "    def load(self, folder, name):\n",
        "        # load keras model weights\n",
        "        self.Actor.Actor.load_weights(os.path.join(folder, f\"{name}_Actor.h5\"))\n",
        "        self.Critic.Critic.load_weights(os.path.join(folder, f\"{name}_Critic.h5\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAAlp0N2ba6r",
        "outputId": "72a94c17-9ac2-4c0a-9d26-0a4fb3eae3bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPUs [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "#tf.config.experimental_run_functions_eagerly(True) # used for debugging and development\n",
        "# tf.compat.v1.disable_eager_execution() # usually using this for fastest performance\n",
        "# tf.keras.utils.disable_interactive_logging()\n",
        "np.random.seed(32)\n",
        "tf.random.set_seed(100)\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(gpus) > 0:\n",
        "\tprint(f'GPUs {gpus}')\n",
        "\ttry: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "\texcept RuntimeError: pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "22kaEloJYIVi"
      },
      "outputs": [],
      "source": [
        "class Shared_Model:\n",
        "    def __init__(self, input_shape, action_space, lr, optimizer, model=\"Dense\"):\n",
        "        X_input = Input(input_shape)\n",
        "        self.action_space = action_space\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        # Shared CNN layers:\n",
        "        if model==\"CNN\":\n",
        "          X = Conv1D(filters=64, kernel_size=6, padding=\"same\", activation=\"tanh\")(X_input)\n",
        "          X = MaxPooling1D(pool_size=2)(X)\n",
        "          X = Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"tanh\")(X)\n",
        "          X = MaxPooling1D(pool_size=2)(X)\n",
        "          X = Flatten()(X)\n",
        "        #EIIE Layers\n",
        "        elif model==\"EIIE\":\n",
        "          X = Conv2D(2, (3, 1))(X_input)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Conv2D(20, (48, 1))(X)\n",
        "          X = Activation('relu')(X)\n",
        "          # print(\"X shape:\", X.shape)\n",
        "\n",
        "          inputB = Input(shape=(1, 50, 10))\n",
        "          modelB = Conv2D(filters=2, kernel_size=(3, 1), activation='relu')(inputB)\n",
        "          modelB = Conv2D(filters=20, kernel_size=(50 - 2, 1), activation='relu')(modelB)\n",
        "          modelB = ZeroPadding2D(padding=((0, 0), (0, 4)))(modelB)\n",
        "          # print(\"modelB shape:\", modelB.shape)\n",
        "          merged = Concatenate(axis=3)([X, modelB])\n",
        "          X = Conv2D(filters=1, kernel_size=(1, 1))(merged)\n",
        "\n",
        "          #output = Dense(self.action_space, activation=\"softmax\")(x)\n",
        "        # Shared LSTM layers:\n",
        "        elif model==\"LSTM\":\n",
        "          X = LSTM(512, return_sequences=True)(X_input)\n",
        "          X = LSTM(256)(X)\n",
        "\n",
        "        # Shared Dense layers:\n",
        "        else:\n",
        "          X = Flatten()(X_input)\n",
        "          X = Dense(512, activation=\"relu\")(X)\n",
        "\n",
        "        # Critic model\n",
        "        V = Dense(512, activation=\"relu\")(X)\n",
        "        V = Dense(256, activation=\"relu\")(V)\n",
        "        V = Dense(64, activation=\"relu\")(V)\n",
        "        value = Dense(1, activation=None)(V)\n",
        "        if model == \"EIIE\":\n",
        "          self.Critic = Model(inputs=[X_input,inputB], outputs = value)\n",
        "        else:\n",
        "          self.Critic = Model(inputs=X_input, outputs = value)\n",
        "        self.Critic.compile(loss=self.critic_PPO2_loss, optimizer=optimizer(learning_rate=lr))\n",
        "\n",
        "        # Actor model\n",
        "        A = Dense(512, activation=\"relu\")(X)\n",
        "        A = Dense(256, activation=\"relu\")(A)\n",
        "        A = Dense(64, activation=\"relu\")(A)\n",
        "        output = Dense(self.action_space, activation=\"softmax\")(A)\n",
        "        if model == \"EIIE\":\n",
        "          self.Actor = Model(inputs = [X_input,inputB], outputs = output)\n",
        "        else:\n",
        "          self.Actor = Model(inputs = X_input, outputs = output)\n",
        "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(learning_rate=lr))\n",
        "\n",
        "    def ppo_loss(self, y_true, y_pred):\n",
        "        # Defined in https://arxiv.org/abs/1707.06347\n",
        "        advantages, prediction_picks = y_true[:, :1], y_true[:, 1:1+self.action_space]\n",
        "        LOSS_CLIPPING = 0.2\n",
        "        ENTROPY_LOSS = 0.001\n",
        "        # Calculate the ratio pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)\n",
        "        # NOTE: we just subtract the logs, which is the same as\n",
        "        # dividing the values and then canceling the log with e^log.\n",
        "        # For why we use log probabilities instead of actual probabilities,\n",
        "        # here's a great explanation:\n",
        "        # https://cs.stackexchange.com/questions/70518/why-do-we-use-the-log-in-gradient-based-reinforcement-algorithms\n",
        "        # TL;DR makes gradient ascent easier behind the scenes.\n",
        "        prob = y_pred\n",
        "        old_prob = prediction_picks\n",
        "\n",
        "        prob = K.clip(prob, 1e-10, 1.0)\n",
        "        old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
        "\n",
        "        ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
        "\n",
        "        p1 = ratio * advantages\n",
        "        p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
        "\n",
        "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
        "\n",
        "        entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
        "        entropy = ENTROPY_LOSS * K.mean(entropy)\n",
        "\n",
        "        total_loss = actor_loss - entropy\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def actor_predict(self, state, order):\n",
        "        if self.model == \"EIIE\":\n",
        "          return self.Actor.predict([state, order])\n",
        "        else:\n",
        "          return self.Actor.predict([state, np.zeros((state.shape[0], 1))])\n",
        "\n",
        "    def critic_PPO2_loss(self, y_true, y_pred):\n",
        "        value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
        "        return value_loss\n",
        "\n",
        "    def critic_predict(self, state, order):\n",
        "        if self.model == \"EIIE\":\n",
        "          return self.Critic.predict([state, order])\n",
        "        else:\n",
        "          return self.Critic.predict([state, np.zeros((state.shape[0], 1))])\n",
        "\n",
        "\n",
        "class Actor_Model:\n",
        "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
        "        X_input = Input(input_shape)\n",
        "        self.action_space = action_space\n",
        "\n",
        "        X = Flatten(input_shape=input_shape)(X_input)\n",
        "        X = Dense(512, activation=\"relu\")(X)\n",
        "        X = Dense(256, activation=\"relu\")(X)\n",
        "        X = Dense(64, activation=\"relu\")(X)\n",
        "        output = Dense(self.action_space, activation=\"softmax\")(X)\n",
        "\n",
        "        self.Actor = Model(inputs = X_input, outputs = output)\n",
        "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(learning_rate=lr))\n",
        "\n",
        "\n",
        "    def ppo_loss(self, y_true, y_pred):\n",
        "        # Defined in https://arxiv.org/abs/1707.06347\n",
        "        advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
        "        LOSS_CLIPPING = 0.2\n",
        "        ENTROPY_LOSS = 0.001\n",
        "\n",
        "        prob = actions * y_pred\n",
        "        old_prob = actions * prediction_picks\n",
        "\n",
        "        prob = K.clip(prob, 1e-10, 1.0)\n",
        "        old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
        "\n",
        "        ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
        "\n",
        "        p1 = ratio * advantages\n",
        "        p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
        "\n",
        "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
        "\n",
        "        entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
        "        entropy = ENTROPY_LOSS * K.mean(entropy)\n",
        "\n",
        "        total_loss = actor_loss - entropy\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def actor_predict(self, state):\n",
        "        return self.Actor.predict(state)\n",
        "\n",
        "class Critic_Model:\n",
        "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
        "        X_input = Input(input_shape)\n",
        "\n",
        "        V = Flatten(input_shape=input_shape)(X_input)\n",
        "        V = Dense(512, activation=\"relu\")(V)\n",
        "        V = Dense(256, activation=\"relu\")(V)\n",
        "        V = Dense(64, activation=\"relu\")(V)\n",
        "        value = Dense(1, activation=None)(V)\n",
        "\n",
        "        self.Critic = Model(inputs=X_input, outputs = value)\n",
        "        self.Critic.compile(loss=self.critic_PPO2_loss, optimizer=optimizer(learning_rate=lr))\n",
        "\n",
        "    def critic_PPO2_loss(self, y_true, y_pred):\n",
        "        value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
        "        return value_loss\n",
        "\n",
        "    def critic_predict(self, state):\n",
        "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LAjeIHvOcM2-"
      },
      "outputs": [],
      "source": [
        "def train_agent(env, agent, visualize=False, train_episodes = 50, training_batch_size=500):\n",
        "    agent.create_writer(env.initial_balance, env.normalize_value, train_episodes) # create TensorBoard writer\n",
        "    total_average = deque(maxlen=100) # save recent 100 episodes net worth\n",
        "    best_average = 0 # used to track best average net worth\n",
        "\n",
        "    for episode in range(train_episodes):\n",
        "        #Reseta o Env/ Reset the env\n",
        "        state, order = env.reset(env_steps_size = training_batch_size)\n",
        "        states, orders, rewards, predictions, dones, next_states, next_orders = [], [], [], [], [], [], []\n",
        "        for t in range(training_batch_size):\n",
        "            # Gets the action to be taken by the agent\n",
        "            prediction = agent.act(state, np.array(order))\n",
        "\n",
        "\n",
        "            prediction = np.squeeze(prediction)\n",
        "\n",
        "            # Perform an action on env\n",
        "            next_state, next_order, reward, done, prices = env.step( prediction)\n",
        "            states.append(np.expand_dims(state, axis=0))\n",
        "            orders.append(np.expand_dims(order, axis=0))\n",
        "\n",
        "            next_states.append(np.expand_dims(next_state, axis=0))\n",
        "            next_orders.append(np.expand_dims(next_order, axis=0))\n",
        "\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "            predictions.append(prediction)\n",
        "            state = next_state\n",
        "            order = next_order\n",
        "\n",
        "        # Train the Model\n",
        "        a_loss, c_loss = agent.replay(states, orders, rewards, predictions, dones, next_states, next_orders)\n",
        "        total_average.append(env.net_worth)\n",
        "        average = np.average(total_average)\n",
        "        rewardFull = np.average(rewards)\n",
        "\n",
        "        agent.writer.add_scalar('Data/average reward', rewardFull, episode)\n",
        "        agent.writer.add_scalar('Data/average net_worth', average, episode)\n",
        "        agent.writer.add_scalar('Data/average net_worth_percent',  round((average-1000)/10), episode)\n",
        "\n",
        "        ubah_value = np.dot(env.quants_ubah, prices).item()  # Extract scalar\n",
        "        diff_value = (env.net_worth - ubah_value).item()  # Ensure scalar\n",
        "\n",
        "\n",
        "        print(\"net worth {} {:.2f} {:.2f} {:.2f} % UBAH {:.2f} diff {:.2f}\".format(episode, env.net_worth, average, (average - 1000) / 10, ubah_value, diff_value))\n",
        "        if episode > len(total_average):\n",
        "          if best_average < average:\n",
        "            best_average = average\n",
        "            print(\"Saving model\")\n",
        "            agent.save(score=\"{:.2f}\".format(best_average), args=[episode, average,  a_loss, c_loss])\n",
        "          agent.save()\n",
        "\n",
        "\n",
        "def test_agent(env, visualize=True, test_episodes=10, testing_batch_size=500):\n",
        "\n",
        "    average_net_worth = 0\n",
        "    average_UBAH = 0\n",
        "    for episode in range(test_episodes):\n",
        "        state, order = env.reset(env_steps_size=testing_batch_size)\n",
        "        old = 0\n",
        "        for t in range(testing_batch_size):\n",
        "            prediction = agent.act(state, np.array(order))\n",
        "            prediction = np.squeeze(prediction)\n",
        "            old = env.net_worth\n",
        "            state, order, reward, done, prices = env.step(prediction)\n",
        "\n",
        "        average_net_worth += env.net_worth\n",
        "        ubah_value = np.dot(env.quants_ubah, prices).item()  # Extract scalar\n",
        "        diff_value = (env.net_worth - ubah_value).item()  # Ensure scalar\n",
        "        average_UBAH += ubah_value\n",
        "\n",
        "        print(\"net_worth: {:.2f} % {:.2f} UBAH {:.2f} diff {:.2f}\".format(\n",
        "            env.net_worth, (env.net_worth - 1000) / 10, ubah_value, diff_value\n",
        "        ))\n",
        "\n",
        "    print(\"average: {:.2f} % {:.2f}\".format(\n",
        "        average_net_worth / test_episodes, average_UBAH / test_episodes\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "id": "hXfuJ7H9ctAi",
        "outputId": "82da64d5-a5b6-4a42-b485-d8c250123d6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shapedp (35511, 14, 4) (35512, 14, 4)\n",
            "shape12345 (25462, 14, 4)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "`tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-75c436a11dca>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtest_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlookback_window_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlookback_window_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlookback_window_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlookback_window_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BNBUSDT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'BNBUSDT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'BNBUSDT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'XRPUSDT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mtest_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#train_agent(train_env, agent, visualize=False, train_episodes=50000, training_batch_size=500)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8e07cd06937d>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(env, agent, visualize, train_episodes, training_batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# Gets the action to be taken by the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1a147deba390>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, order)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Use the network to predict the next action to take, using the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-9cc2634f048f>\u001b[0m in \u001b[0;36mactor_predict\u001b[0;34m(self, state, order)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mactor_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"EIIE\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n\u001b[0m\u001b[1;32m    504\u001b[0m                          \"iteration in eager mode or within tf.function.\")\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: `tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function."
          ]
        }
      ],
      "source": [
        "ETHUSDT = pd.read_csv('/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/ETHUSDT.csv')\n",
        "BTCUSDT = pd.read_csv('/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/BTCUSDT.csv')\n",
        "BNBUSDT = pd.read_csv('/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/BNBUSDT.csv')\n",
        "XRPUSDT = pd.read_csv('/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/XRPUSDT.csv')\n",
        "ETHUSDT_norm = pd.read_csv('/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/ETHUSDT_norm.csv')\n",
        "BTCUSDT_norm = pd.read_csv('/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/BTCUSDT_norm.csv')\n",
        "BNBUSDT_norm = pd.read_csv('/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/BNBUSDT_norm.csv')\n",
        "XRPUSDT_norm = pd.read_csv('/root/.cache/kagglehub/datasets/nandodmelo/cripto-hour/versions/1/XRPUSDT_norm.csv')\n",
        "\n",
        "train_df = []\n",
        "train_df_norm = []\n",
        "train_df.append(ETHUSDT.values)\n",
        "train_df.append(BTCUSDT.values)\n",
        "train_df.append(BNBUSDT.values)\n",
        "train_df.append(XRPUSDT.values)\n",
        "train_df_norm.append(XRPUSDT_norm.values)\n",
        "train_df_norm.append(BTCUSDT_norm.values)\n",
        "train_df_norm.append(BNBUSDT_norm.values)\n",
        "train_df_norm.append(XRPUSDT_norm.values)\n",
        "\n",
        "xa = np.copy(np.moveaxis(np.array(train_df),0,-1))\n",
        "x_norm = np.copy(np.moveaxis(np.array(train_df_norm),0,-1))\n",
        "lookback_window_size = 50\n",
        "print('shapedp',x_norm.shape, xa.shape)\n",
        "train_df = xa[:-10000-lookback_window_size]\n",
        "train_df_norm = x_norm[:-10000-lookback_window_size]\n",
        "test_df = xa[-10000:] # 30 days\n",
        "test_df_norm = x_norm[-10000:]\n",
        "\n",
        "\n",
        "\n",
        "print('shape12345',train_df.shape)\n",
        "model = 'EIIE'\n",
        "train_env = CustomEnv(train_df, train_df_norm, lookback_window_size=lookback_window_size, model=model)\n",
        "test_env = CustomEnv(test_df, test_df_norm,lookback_window_size=lookback_window_size, model=model)\n",
        "agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=5, stocks=['BNBUSDT','BNBUSDT','BNBUSDT','XRPUSDT'], optimizer=Adam, batch_size = 32, model=model, shape = x_norm.shape)\n",
        "train_agent(train_env, agent, visualize=False, train_episodes=50, training_batch_size=500)\n",
        "test_agent(test_env, visualize=False, test_episodes=10, testing_batch_size=500)\n",
        "#train_agent(train_env, agent, visualize=False, train_episodes=50000, training_batch_size=500)\n",
        "#test_agent(test_env, visualize=False, test_episodes=10000, testing_batch_size=500)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
